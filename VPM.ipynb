{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocal Pitch Modulator NN Training \n",
    "This is the notebook used to train the Vocal Pitch Modulator.\n",
    "\n",
    "This notebook is split into two sections. The first section goes through in detail (with plots and prints) how the data is organized. The second section is the portion that makes use of the data to train our timbre encoder\n",
    "\n",
    "## Global variables/Imports\n",
    "Run these cells before running either of the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "%aimport VPM\n",
    "from VPM import *\n",
    "%aimport Utils\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that should not change without the dataset being changed\n",
    "n_pitches = 16\n",
    "n_vowels = 12\n",
    "n_people = 3\n",
    "\n",
    "# These dictionaries are more for reference than anything\n",
    "label_to_vowel = { 0: \"bed\",  1: \"bird\",   2: \"boat\",  3: \"book\", \n",
    "                   4: \"cat\",  5: \"dog\",    6: \"feet\",  7: \"law\",  \n",
    "                   8: \"moo\",  9: \"nut\",   10: \"pig\",  11: \"say\" }\n",
    "\n",
    "vowel_to_label = { \"bed\": 0,  \"bird\": 1,  \"boat\":  2, \"book\":  3,\n",
    "                   \"cat\": 4,  \"dog\":  5,  \"feet\":  6, \"law\":   7,\n",
    "                   \"moo\": 8,  \"nut\":  9,  \"pig\":  10, \"say\":  11}\n",
    "\n",
    "noteidx_to_pitch = {  0: \"A2\",   1: \"Bb2\",  2: \"B2\",   3: \"C3\",\n",
    "                      4: \"Db3\",  5: \"D3\",   6: \"Eb3\",  7: \"E3\", \n",
    "                      8: \"F3\",   9: \"Gb3\", 10: \"G3\",  11: \"Ab3\",\n",
    "                     12: \"A3\",  13: \"Bb3\", 14: \"B3\",  15: \"C4\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Walkthrough\n",
    "This first section demonstrates how the data is put into data structures for training. You can skip to the second section and simply run the first cell there to generate the data structures, there is no need to execute the cells in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data references\n",
    "Read the reference csv to relevant data structure.\n",
    "\n",
    "`data_ref_list` is the list of filenames in the dataset in a 3d array format.\n",
    "A specific file is accessed with `data_ref_list[vowel_idx][pitch_idx][person_idx]`.\n",
    "\n",
    "`flat_data_ref_list` is the list of filenames in the dataset as a 1d array. To access a specific file, use `flat_data_ref_list[flat_idx(vowel, pitch, person)]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# e.g. data_list[vowel_to_label[\"dog\"]][5][1]\n",
    "data_ref_list = create_data_ref_list(os.path.join(\"Data\", 'dataset_files.csv'),\n",
    "                            n_pitches, n_vowels, n_people)\n",
    "# print(data_ref_list)\n",
    "# e.g. flat_data_ref_list[flat_idx(3, 1, 2)]\n",
    "flat_data_ref_list = flatten_3d_array(data_ref_list, \n",
    "                                      n_vowels, n_pitches, n_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the accessor functions used to compute indices from flat to 3d and vice versa.\n",
    "\n",
    "`flat_idx` returns a `flat_idx`, given a `(vowel, pitch, person)`, while `nd_idx` returns `vowel, pitch, person`, given a `flat_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a flat_idx, given a vowel, pitch, person\n",
    "flat_idx = lambda vowel, pitch, person: flat_array_idx(\n",
    "    vowel, pitch, person, n_vowels, n_pitches, n_people)\n",
    "# Returns vowel, pitch, person, given a flat_idx\n",
    "nd_idx = lambda idx: nd_array_idx(idx, n_vowels, n_pitches, n_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Data ref list ({}):\".format(len(flat_data_ref_list)), \n",
    "      flat_data_ref_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-label Pitch Index pairs\n",
    "Generate the data-label pitch index pairs. This is an array where each element is a 3-tuple of `[shift_amt, input_pitch_idx, label_pitch_iIdx]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_label_pairs, _ = create_data_label_pairs(n_pitches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total data-label pairs ({}):\".format(len(data_label_pairs)), \n",
    "      data_label_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get All .wav Data\n",
    "Get the wav file data into a single matrix, where each element `all_wav_data[idx]` is the wavfile content of the file at `flat_data_ref_list[idx]`. To retrieve the 3d indices of a specific index, use `vowel, pitch, person = nd_idx(idx)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wav_data = load_wav_files(os.path.join(\"Data\", \"dataset\"), \n",
    "                              flat_data_ref_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All wav data length: {}\\nTrack length: {}\".format(\n",
    "      all_wav_data.shape, all_wav_data[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create all spectrograms\n",
    "Get the spectrograms for each wav in `all_wav_data`. The spectrogram at `all_spectrograms[idx]` is the spectrogram of the wav at `all_wav_data[idx]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectrograms = np.array([ stft(waveform, plot=False) \n",
    "                              for waveform in all_wav_data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All spectrograms has shape: {} (n_wavs, n_freq_bins, n_windows)\\n\"\n",
    "      .format(all_spectrograms.shape))\n",
    "\n",
    "print(\"FFT Spectrogram of vowel 4, pitch 3, person 2 ({}):\"\n",
    "      .format(flat_data_ref_list[flat_idx(4, 3, 2)]))\n",
    "plot_ffts_spectrogram(all_spectrograms[flat_idx(4, 3, 2)], sample_rate,\n",
    "                      flat_data_ref_list[flat_idx(4, 3, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mel Spectrograms and MFCC\n",
    "Get the mel spectrograms/MFCC for each ffts (spectrogram) in `all_spectrograms` (similar indexing as above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mels, all_mfcc = map(np.array, map(list, zip(*\n",
    "                         [ ffts_to_mel(ffts, n_mels = 128) \n",
    "                           for ffts in all_spectrograms ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All mels has shape: {} (n_wavs, n_mels, n_windows)\"\n",
    "      .format(all_mels.shape))\n",
    "print(\"All mfccs has shape: {} (n_wavs, n_mfcc, n_windows)\\n\"\n",
    "      .format(all_mfcc.shape))\n",
    "\n",
    "print(\"Mel Spectrogram of vowel 4, pitch 3, person 2 ({}):\"\n",
    "      .format(flat_data_ref_list[flat_idx(4, 3, 2)]))\n",
    "plot_mel_spectrogram(all_mels[flat_idx(4, 3, 2)], sample_rate,\n",
    "                     flat_data_ref_list[flat_idx(4, 3, 2)])\n",
    "print(\"MFCC of vowel 4, pitch 3, person 2 ({}):\"\n",
    "      .format(flat_data_ref_list[flat_idx(4, 3, 2)]))\n",
    "plot_mfcc(all_mfcc[flat_idx(4, 3, 2)], sample_rate,\n",
    "          flat_data_ref_list[flat_idx(4, 3, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Training - Timbre Encoder\n",
    "This section is where training for the Timbre encoder is done.\n",
    "\n",
    "### Imports\n",
    "Run these cells before running the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%aimport ANN\n",
    "from ANN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Used to tune the ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_mfcc = 20\n",
    "n_hid1 = 12\n",
    "n_timb = 4\n",
    "lr     = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "This is all the code that was explained in the Data Walkthrough above. It generates data structures to hold all wav file data, spectrograms, mel spectra and MFCC data for all wav files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File reference lists\n",
    "data_ref_list = create_data_ref_list(os.path.join(\"Data\", 'dataset_files.csv'),\n",
    "                            n_pitches, n_vowels, n_people)\n",
    "# flat_data_ref_list[flat_idx(vowel, pitch, person)]\n",
    "flat_data_ref_list = flatten_3d_array(data_ref_list, \n",
    "                                      n_vowels, n_pitches, n_people)\n",
    "\n",
    "# File reference list accessors\n",
    "# Returns a flat_idx, given a vowel, pitch, person\n",
    "flat_idx = lambda vowel, pitch, person: flat_array_idx(\n",
    "    vowel, pitch, person, n_vowels, n_pitches, n_people)\n",
    "# Returns vowel, pitch, person, given a flat_idx\n",
    "nd_idx = lambda idx: nd_array_idx(idx, n_vowels, n_pitches, n_people)\n",
    "\n",
    "# Data-label pairs for pitch-shift training - not used here\n",
    "# data_label_pairs, _ = create_data_label_pairs(n_pitches)\n",
    "\n",
    "# wav, spectrogram, mels, mfcc for each file in flat_data_ref_list\n",
    "# wav_data:     (576, ~29400)  (n_wavs, n_samples)\n",
    "# spectrograms: (576, 513, 58) (n_wavs, n_freq_bins, n_windows)\n",
    "# mels:         (576, 128, 58) (n_wavs, n_mels, n_windows)\n",
    "# mfccs:        (576, 20, 58)  (n_wavs, n_mfcc, n_windows)\n",
    "all_wav_data = load_wav_files(os.path.join(\"Data\", \"dataset\"), \n",
    "                              flat_data_ref_list)\n",
    "all_spectrograms = np.array([ stft(waveform, plot=False) \n",
    "                              for waveform in all_wav_data ])\n",
    "all_mels, all_mfcc = map(np.array, map(list, zip(*\n",
    "                         [ ffts_to_mel(ffts, n_mels = n_mels, n_mfcc = n_mfcc) \n",
    "                           for ffts in all_spectrograms ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Label Structuring\n",
    "This puts together the actual data-label pairs to be fed into the ANN.\n",
    "\n",
    "Generate `data` and `labels` from `all_mfcc` and using `nd_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_files, n_mfcc_dummy, n_windows = all_mfcc.shape\n",
    "\n",
    "# vowel_labels: (576) (n_wavs)\n",
    "all_vowel_labels, _, _ = map(np.array, map(list, zip(*\n",
    "                         [ nd_idx(idx) \n",
    "                           for idx in range(len(flat_data_ref_list)) ])))\n",
    "# data:   (33408, 20) (n_wavs * n_windows, n_mfcc)\n",
    "# labels: (33408) (n_wavs * n_windows)\n",
    "data = np.array([ all_mfcc[wav_file_idx][:, window_idx] \n",
    "                  for wav_file_idx in range(n_files) \n",
    "                  for window_idx in range(n_windows) ])\n",
    "labels = np.array([ all_vowel_labels[wav_file_idx]\n",
    "                    for wav_file_idx in range(n_files)\n",
    "                    for window_idx in range(n_windows) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes - verify that the mfcc have been arranged in order of\n",
    "# wav_idx, win_idx, mel_feature_idx\n",
    "for wav_idx in range(n_files):\n",
    "    for win_idx in range(n_windows):\n",
    "        for m in range(n_mfcc_dummy):\n",
    "            assert data[wav_idx * n_windows + win_idx][m] == \\\n",
    "                   all_mfcc[wav_idx][m][win_idx]\n",
    "for wav_idx in range(n_files):\n",
    "    for win_idx in range(n_windows):\n",
    "        assert labels[wav_idx * n_windows + win_idx] == \\\n",
    "               all_vowel_labels[wav_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into `train` and `test`, and convert to Torch tensors of the correct types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (25056, 20) (25056) \n",
    "# X_val, Y_val:     (8352, 20) (8352)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, stratify=labels, random_state=0)\n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "# Used as index, so it is long\n",
    "Y_train = Y_train.long(); Y_val = Y_val.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timbre-Encoder\n",
    "This takes MFCC (and mel-spectrograms in future?), and tries to identify the vowel spoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model \n",
    "# model = TimbreEncoder(n_mfcc=n_mfcc, n_hid1=n_hid1, n_timb=n_timb, n_vowels=n_vowels)\n",
    "model = TimbreEncoder()\n",
    "# Define loss \n",
    "loss_fn = F.cross_entropy\n",
    "# Define optimizer \n",
    "opt = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU Version (Only run if GPU is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if possible\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train_gpu = X_train.to(device)\n",
    "Y_train_gpu = Y_train.to(device)\n",
    "X_val_gpu = X_val.to(device)\n",
    "Y_val_gpu = Y_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# model = model.to(device) \n",
    "# Define optimizer \n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = fit(X_train_gpu, Y_train_gpu, X_val_gpu, Y_val_gpu, \n",
    "           model, opt, loss_fn, epochs=5000, print_graph = True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-GPU Version (Run if GPU is not available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = fit(X_train, Y_train, X_val, Y_val, model, opt, loss_fn, epochs=500, print_graph = True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
