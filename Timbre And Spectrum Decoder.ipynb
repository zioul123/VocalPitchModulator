{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimbreXSpectrum Decoder\n",
    "This is the notebook used to train the Vocal Pitch Modulator.\n",
    "\n",
    "This notebook makes use of the timbre encoder and spectrum data to train our decoder.\n",
    "\n",
    "## Global variables/Imports\n",
    "Run these cells before running either of the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%aimport VPM\n",
    "from VPM import *\n",
    "%aimport Utils\n",
    "from Utils import *\n",
    "%aimport ANN\n",
    "from ANN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that should not change without the dataset being changed\n",
    "n_pitches = 16\n",
    "n_vowels = 12\n",
    "n_people = 3\n",
    "\n",
    "# These dictionaries are more for reference than anything\n",
    "label_to_vowel = { 0: \"bed\",  1: \"bird\",   2: \"boat\",  3: \"book\", \n",
    "                   4: \"cat\",  5: \"dog\",    6: \"feet\",  7: \"law\",  \n",
    "                   8: \"moo\",  9: \"nut\",   10: \"pig\",  11: \"say\" }\n",
    "\n",
    "vowel_to_label = { \"bed\": 0,  \"bird\": 1,  \"boat\":  2, \"book\":  3,\n",
    "                   \"cat\": 4,  \"dog\":  5,  \"feet\":  6, \"law\":   7,\n",
    "                   \"moo\": 8,  \"nut\":  9,  \"pig\":  10, \"say\":  11}\n",
    "\n",
    "noteidx_to_pitch = {  0: \"A2\",   1: \"Bb2\",  2: \"B2\",   3: \"C3\",\n",
    "                      4: \"Db3\",  5: \"D3\",   6: \"Eb3\",  7: \"E3\", \n",
    "                      8: \"F3\",   9: \"Gb3\", 10: \"G3\",  11: \"Ab3\",\n",
    "                     12: \"A3\",  13: \"Bb3\", 14: \"B3\",  15: \"C4\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Used to tune the data generation and ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_mfcc = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "This is all the code that was explained in the Data Walkthrough. It generates data structures to hold all wav file data, spectrograms, mel spectra and MFCC data for all wav files.\n",
    "\n",
    "For diagram-visualization of the data set, refer to the [readme](https://github.com/zioul123/VocalPitchModulator/blob/master/README.md).\n",
    "\n",
    "The first cell involves 3d arrays, while the second cell involves flattened arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File reference lists\n",
    "data_ref_list = create_data_ref_list(os.path.join(\"Data\", 'dataset_files.csv'),\n",
    "                            n_pitches, n_vowels, n_people)\n",
    "# flat_data_ref_list[flat_ref_idx(vowel, pitch, person)]\n",
    "flat_data_ref_list = flatten_3d_array(data_ref_list, \n",
    "                                      n_vowels, n_pitches, n_people)\n",
    "\n",
    "# File reference list accessors\n",
    "# Returns a flat_ref_idx, given a vowel, pitch, person\n",
    "flat_ref_idx = lambda vowel, pitch, person: flat_3d_array_idx(\n",
    "    vowel, pitch, person, n_vowels, n_pitches, n_people)\n",
    "# Returns vowel, pitch, person, given a flat_ref_idx\n",
    "nd_ref_idx = lambda idx: nd_array_idx(idx, n_vowels, n_pitches, n_people)\n",
    "\n",
    "# Data-label pairs for pitch-shift training\n",
    "data_label_pairs_flat, data_label_pairs_dict = create_data_label_pairs(n_pitches)\n",
    "\n",
    "# wav, spectrogram, mels, mfcc for each file in flat_data_ref_list. n_windows is 115 with 75% overlap.\n",
    "# wav_data:     (576, ~29400)  (n_wavs, n_samples)\n",
    "# spectrograms: (576, 513, 58) (n_wavs, n_freq_bins, n_windows)\n",
    "# mels:         (576, 128, 58) (n_wavs, n_mels, n_windows)\n",
    "# mfccs:        (576, 20, 58)  (n_wavs, n_mfcc, n_windows)\n",
    "all_wav_data = load_wav_files(os.path.join(\"Data\", \"dataset\"), \n",
    "                              flat_data_ref_list)\n",
    "all_spectrograms = np.array([ stft(waveform, overlap=.5, plot=False) \n",
    "                              for waveform in all_wav_data ])\n",
    "all_mels, all_mfcc = map(np.array, map(list, zip(*\n",
    "                         [ ffts_to_mel(ffts, n_mels = n_mels, n_mfcc = n_mfcc) \n",
    "                           for ffts in all_spectrograms ])))\n",
    "\n",
    "n_files, _, n_windows = all_mfcc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattened data\n",
    "\n",
    "# Create data accessor for flat arrays\n",
    "flat_data_idx = lambda wav_idx, win_idx: flat_2d_array_idx(\n",
    "    wav_idx, win_idx, n_files, n_windows)\n",
    "\n",
    "# Create flat mel spectra \n",
    "# flat_mels: (33408, 128)\n",
    "flat_mels_prenorm = np.array([ all_mels[wav_file_idx][:, window_idx] \n",
    "                               for wav_file_idx in range(n_files) \n",
    "                               for window_idx in range(n_windows) ])\n",
    "# Normalize each mel spectrum to [0, 1]\n",
    "flat_mels, mels_scales = normalize_rows(flat_mels_prenorm, NormMode.NONNEG_TO_ZERO_ONE)\n",
    "\n",
    "# Create timbre features by calling TE encoding on mfccs\n",
    "# flat_mfcc: (33408, 20)\n",
    "flat_mfcc_prenorm = np.array([ all_mfcc[wav_file_idx][:, window_idx]\n",
    "                               for wav_file_idx in range(n_files)\n",
    "                               for window_idx in range(n_windows) ])\n",
    "# Normalize each mfcc (i.e. rows of the all_mfcc array) to [0,1] - USE FOR THE VAE (TimbreVAE)\n",
    "flat_mfcc, mfcc_scales = normalize_rows(flat_mfcc_prenorm, NormMode.REAL_TO_ZERO_ONE)\n",
    "flat_mfcc = torch.Tensor(flat_mfcc)\n",
    "\n",
    "# Load the timbre encoder\n",
    "n_mfcc = 20; n_hid = 10; n_timb = 4\n",
    "TE = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "TE.load_state_dict(torch.load(os.path.join('model_data', 'TimbreVAE_Ideal.pt')))\n",
    "TE.eval()\n",
    "# Call the timbre VAE's encode \n",
    "# flat_timbre: (33408, 4)\n",
    "flat_timbre = TE.get_z(flat_mfcc).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training - \"Autoencoder\" Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Label Structuring\n",
    "This puts together the actual data-label pairs to be fed into the ANN.\n",
    "\n",
    "`data` is generated from timbre features (which requires the use of the TimbreVAE) and mel spectrums. `labels` are abs(fft) spectrograms. At this stage, pre-training, we simply want to make the NN learn to output fourier spectra.\n",
    "\n",
    "Note that mel_spectra/mfcc of abs(fft) are equivalent to those of fft itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files, n_mfcc_dummy, n_windows = all_mfcc.shape\n",
    "\n",
    "# Create labels\n",
    "# spectrograms: (576, 513, 58) (n_wavs, n_freq_bins, n_windows)\n",
    "all_abs_spectrograms = np.abs(all_spectrograms)\n",
    "\n",
    "# data:   (33408, 132) (n_wavs * n_windows, n_timb + n_mels)\n",
    "# labels: (33408, 513) (n_wavs * n_windows, n_fft / 2 + 1)\n",
    "data = np.concatenate((flat_timbre, flat_mels), axis=1)\n",
    "labels_prenorm = np.array([ all_abs_spectrograms[wav_file_idx][:, window_idx]\n",
    "                            for wav_file_idx in range(n_files)\n",
    "                            for window_idx in range(n_windows) ])\n",
    "labels, label_scales = normalize_rows(labels_prenorm, NormMode.NONNEG_TO_ZERO_ONE)\n",
    "print(data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(labels_prenorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't work after normalization.\n",
    "# # For testing purposes - VERY SLOW\n",
    "# # Verify that the timbre and mels have been arranged in order of\n",
    "# # wav_idx, win_idx, [timbre then mel_idx]\n",
    "# for wav_idx in range(n_files):\n",
    "#     for win_idx in range(n_windows):\n",
    "#         for t in range(n_timb):\n",
    "#             assert data[flat_data_idx(wav_idx, win_idx)][t] == \\\n",
    "#                    flat_timbre[flat_data_idx(wav_idx, win_idx)][t]\n",
    "#         for m in range(n_mels):\n",
    "#             assert data[flat_data_idx(wav_idx, win_idx)][m + n_timb] == \\\n",
    "#                    all_mels[wav_idx][m][win_idx]\n",
    "\n",
    "# # Verify that the labels are arranged in order of wav_idx, win_idx, fft_bin_idx\n",
    "# for wav_idx in range(n_files):\n",
    "#     for win_idx in range(n_windows):\n",
    "#         for f in range(513):\n",
    "#             assert labels[flat_data_idx(wav_idx, win_idx)][f] == \\\n",
    "#                    all_abs_spectrograms[wav_idx][f][win_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into `train` and `test`, and convert to Torch tensors of the correct types. Run **only one of these cells.**\n",
    "\n",
    "First method (**not-recommended**, simple): Random sampling to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (25056, 20) (25056) \n",
    "# X_val, Y_val:     (8352, 20) (8352)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, random_state=0)\n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "Y_train = Y_train.float(); Y_val = Y_val.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second method **(recommended)**: 1 person from each wav will be the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (22272, 132) (22272, 513) \n",
    "# X_val, Y_val:     (11136, 132) (11136, 513)\n",
    "X_train = []; X_val = []; Y_train = []; Y_val = []\n",
    "for vow_idx in range(n_vowels):\n",
    "    for pit_idx in range(n_pitches):\n",
    "        # Choose the person for this pitch/vowel to be used as test data\n",
    "        test_pid = int(np.random.rand() * 3)\n",
    "        for pid_idx in range(n_people):\n",
    "            wav_idx = flat_ref_idx(vow_idx, pit_idx, pid_idx)\n",
    "            if (pid_idx != test_pid):\n",
    "                for win_idx in range(n_windows):\n",
    "                    X_train.append(data[flat_data_idx(wav_idx, win_idx)])\n",
    "                    Y_train.append(labels[flat_data_idx(wav_idx, win_idx)])\n",
    "            else:\n",
    "                for win_idx in range(n_windows):\n",
    "                    X_val.append(data[flat_data_idx(wav_idx, win_idx)])\n",
    "                    Y_val.append(labels[flat_data_idx(wav_idx, win_idx)])  \n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "Y_train = Y_train.float(); Y_val = Y_val.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder stage - Timb + Mel -> FFT (decoder style, no pitch shift)\n",
    "\n",
    "This takes a timbre vector and mel spectrum, and tries to recreate the original FFT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 132; n_hid2 = 386; n_hid = 260; n_ffts = 513; \n",
    "lr = 0.2; n_epochs = 500; batch_size=22272//8;\n",
    "\n",
    "# Training model \n",
    "model = TimbreMelDecoder(n_input=n_input, n_hid=n_hid, n_hid2=n_hid2, n_ffts=n_ffts)\n",
    "# Define loss \n",
    "loss_fn = nn.MSELoss()\n",
    "# Just trying out some different loss functions that consider log\n",
    "loss_fn = nn.KLDivLoss()\n",
    "# loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# Define optimizer \n",
    "# opt = optim.SGD(model.parameters(), lr=lr)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "train_loss, val_loss = model.train_func(X_train, Y_train, X_val, Y_val, model, opt,\n",
    "                        loss_fn, batch_size=batch_size, epochs=n_epochs, print_graph=True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(train_loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreDecoder_{}_{}_{}_{}_{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_hid, n_hid2, n_ffts, train_loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimbreMelDecoder(n_input=n_input, n_hid=n_hid, n_hid2=n_hid2, n_ffts=n_ffts)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "# model.to(device)\n",
    "\n",
    "data_tensor, label_tensor = map(torch.tensor, (data, labels))\n",
    "data_tensor = data_tensor.float(); label_tensor = label_tensor.long(); \n",
    "# data_tensor = data_tensor.to(device); label_tensor = label_tensor.to(device)\n",
    "predictions = model(data_tensor).detach().numpy()\n",
    "\n",
    "\n",
    "x_axis = librosa.core.fft_frequencies(sample_rate, 1024)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_axis, predictions[30000], label='Prediction', linewidth=1)\n",
    "plt.plot(x_axis, label_tensor[30000], label='Label', linewidth=1)\n",
    "plt.xlim(0, 5000)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('abs(fft)')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Output-Label for data point 30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# These are rough work\n",
    "No guarantee that this cell even works. Tests to see how bad reconstructed sound without phase information is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav_data:     (576, ~29400)  (n_wavs, n_samples)\n",
    "# spectrograms: (576, 513, 58) (n_wavs, n_freq_bins, n_windows)\n",
    "# mels:         (576, 128, 58) (n_wavs, n_mels, n_windows)\n",
    "# mfccs:        (576, 20, 58)  (n_wavs, n_mfcc, n_windows)\n",
    "# all_wav_data = load_wav_files(os.path.join(\"Data\", \"dataset\"), \n",
    "#                               flat_data_ref_list)\n",
    "# all_spectrograms = np.array([ stft(waveform, plot=False) \n",
    "#                               for waveform in all_wav_data ])\n",
    "# all_mels, all_mfcc = map(np.array, map(list, zip(*\n",
    "#                          [ ffts_to_mel(ffts, n_mels = n_mels, n_mfcc = n_mfcc) \n",
    "#                            for ffts in all_spectrograms ])))\n",
    "\n",
    "# Vowel, pitch, person\n",
    "CAT_D3_IDX = flat_ref_idx(4, 5, 0)\n",
    "\n",
    "# The original\n",
    "wavform_original = np.array(all_wav_data[CAT_D3_IDX] * np.iinfo(np.int16).max, dtype=np.int16)\n",
    "sio.wavfile.write('Original {} {}.wav'.format(label_to_vowel[4], noteidx_to_pitch[5]), 44100, wavform_original)\n",
    "print(wavform_original.shape)\n",
    "\n",
    "# The reconstructed signal - it's normalized so it's louder, but no obvious differences\n",
    "\n",
    "istft_waveform = istft(all_spectrograms[CAT_D3_IDX], overlap=.75)\n",
    "print(np.array(istft_waveform * np.iinfo(np.int16).max, dtype=np.int16))\n",
    "sio.wavfile.write('ISTFT {} {}.wav'.format(label_to_vowel[4], noteidx_to_pitch[5]), 44100, istft_waveform)\n",
    "plt.plot(istft_waveform)\n",
    "print(istft_waveform.shape)\n",
    "\n",
    "# Realed signal with ISTFT - sounds very bad\n",
    "istft_abs_waveform = istft(np.abs(all_spectrograms[CAT_D3_IDX]), overlap=.75)\n",
    "print(np.array(abs_waveform * np.iinfo(np.int16).max, dtype=np.int16))\n",
    "sio.wavfile.write('ISTFT(ABS) {} {}.wav'.format(label_to_vowel[4], noteidx_to_pitch[5]), 44100, istft_abs_waveform)\n",
    "plt.figure()\n",
    "plt.plot(istft_abs_waveform)\n",
    "print(istft_abs_waveform.shape)\n",
    "\n",
    "# Realed signal with griffinlim - noisy but acceptable\n",
    "gl_abs_waveform = librosa.griffinlim(np.abs(all_spectrograms[CAT_D3_IDX]), win_length=1024)\n",
    "print(np.array(abs_waveform * np.iinfo(np.int16).max, dtype=np.int16))\n",
    "sio.wavfile.write('GL(ABS) {} {}.wav'.format(label_to_vowel[4], noteidx_to_pitch[5]), 44100, gl_abs_waveform)\n",
    "plt.figure()\n",
    "plt.plot(gl_abs_waveform)\n",
    "print(gl_abs_waveform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
