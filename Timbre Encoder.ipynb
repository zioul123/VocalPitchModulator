{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timbre Encoder\n",
    "This is the notebook used to train the Vocal Pitch Modulator.\n",
    "\n",
    "This notebook makes use of the data to train our timbre encoder.\n",
    "\n",
    "There are two models here, the first is a vowel classifier, that takes in an MFCC and outputs a vowel, and the second is a VAE that takes in an MFCC, reduces its dimensionality, and attempts to reconstruct the provided MFCC.\n",
    "\n",
    "## Global variables/Imports\n",
    "Run these cells before running either of the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%aimport VPM\n",
    "from VPM import *\n",
    "%aimport Utils\n",
    "from Utils import *\n",
    "%aimport ANN\n",
    "from ANN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that should not change without the dataset being changed\n",
    "n_pitches = 16\n",
    "n_vowels = 12\n",
    "n_people = 3\n",
    "\n",
    "# These dictionaries are more for reference than anything\n",
    "label_to_vowel = { 0: \"bed\",  1: \"bird\",   2: \"boat\",  3: \"book\", \n",
    "                   4: \"cat\",  5: \"dog\",    6: \"feet\",  7: \"law\",  \n",
    "                   8: \"moo\",  9: \"nut\",   10: \"pig\",  11: \"say\" }\n",
    "\n",
    "vowel_to_label = { \"bed\": 0,  \"bird\": 1,  \"boat\":  2, \"book\":  3,\n",
    "                   \"cat\": 4,  \"dog\":  5,  \"feet\":  6, \"law\":   7,\n",
    "                   \"moo\": 8,  \"nut\":  9,  \"pig\":  10, \"say\":  11}\n",
    "\n",
    "noteidx_to_pitch = {  0: \"A2\",   1: \"Bb2\",  2: \"B2\",   3: \"C3\",\n",
    "                      4: \"Db3\",  5: \"D3\",   6: \"Eb3\",  7: \"E3\", \n",
    "                      8: \"F3\",   9: \"Gb3\", 10: \"G3\",  11: \"Ab3\",\n",
    "                     12: \"A3\",  13: \"Bb3\", 14: \"B3\",  15: \"C4\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Used to tune the data generation and ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_mfcc = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "This is all the code that was explained in the Data Walkthrough. It generates data structures to hold all wav file data, spectrograms, mel spectra and MFCC data for all wav files.\n",
    "\n",
    "For diagram-visualization of the data set, refer to the [readme](https://github.com/zioul123/VocalPitchModulator/blob/master/README.md).\n",
    "\n",
    "For the classifier, MFCC are normalized by row to `[-1, 1]`, as the classifer can learn it better.\n",
    "For the VAE, MFCC are normalized by row to `[0, 1]`, to allow the VAE to output it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File reference lists\n",
    "data_ref_list = create_data_ref_list(os.path.join(\"Data\", 'dataset_files.csv'),\n",
    "                            n_pitches, n_vowels, n_people)\n",
    "# flat_data_ref_list[flat_ref_idx(vowel, pitch, person)]\n",
    "flat_data_ref_list = flatten_3d_array(data_ref_list, \n",
    "                                      n_vowels, n_pitches, n_people)\n",
    "\n",
    "# File reference list accessors\n",
    "# Returns a flat_ref_idx, given a vowel, pitch, person\n",
    "flat_ref_idx = lambda vowel, pitch, person: flat_3d_array_idx(\n",
    "    vowel, pitch, person, n_vowels, n_pitches, n_people)\n",
    "# Returns vowel, pitch, person, given a flat_ref_idx\n",
    "nd_ref_idx = lambda idx: nd_array_idx(idx, n_vowels, n_pitches, n_people)\n",
    "\n",
    "# Data-label pairs for pitch-shift training - not used here\n",
    "# data_label_pairs, _ = create_data_label_pairs(n_pitches)\n",
    "\n",
    "# wav, spectrogram, mels, mfcc for each file in flat_data_ref_list\n",
    "# wav_data:     (576, ~29400)  (n_wavs, n_samples)\n",
    "# spectrograms: (576, 513, 58) (n_wavs, n_freq_bins, n_windows)\n",
    "# mels:         (576, 128, 58) (n_wavs, n_mels, n_windows)\n",
    "# mfccs:        (576, 20, 58)  (n_wavs, n_mfcc, n_windows)\n",
    "all_wav_data = load_wav_files(os.path.join(\"Data\", \"dataset\"), \n",
    "                              flat_data_ref_list)\n",
    "all_spectrograms = np.array([ stft(waveform, plot=False) \n",
    "                              for waveform in all_wav_data ])\n",
    "all_mels, all_mfcc = map(np.array, map(list, zip(*\n",
    "                         [ ffts_to_mel(ffts, n_mels = n_mels, n_mfcc = n_mfcc) \n",
    "                           for ffts in all_spectrograms ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment one of these lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize each mfcc (i.e. rows of the all_mfcc array) to [-1,1] - USE FOR THE CLASSIFER (TimbreEncoder)\n",
    "# all_mfcc = normalize_rows(all_mfcc, NormMode.REAL_TO_NEG_ONE_ONE)\n",
    "#Normalize each mfcc (i.e. rows of the all_mfcc array) to [0,1] - USE FOR THE VAE (TimbreVAE)\n",
    "all_mfcc = normalize_rows(all_mfcc, NormMode.REAL_TO_ZERO_ONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Label Structuring\n",
    "This puts together the actual data-label pairs to be fed into the ANN.\n",
    "\n",
    "Generate `data` and `labels` from `all_mfcc` and using `nd_ref_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_files, n_mfcc_dummy, n_windows = all_mfcc.shape\n",
    "\n",
    "# vowel_labels: (576) (n_wavs)\n",
    "all_vowel_labels, _, _ = map(np.array, map(list, zip(*\n",
    "                         [ nd_ref_idx(idx) \n",
    "                           for idx in range(len(flat_data_ref_list)) ])))\n",
    "\n",
    "# Returns a flat 2d idx, given a wavfile index and a window index\n",
    "flat_data_idx = lambda wav_idx, win_idx: flat_2d_array_idx(\n",
    "    wav_idx, win_idx, n_files, n_windows)\n",
    "\n",
    "# data:   (33408, 20) (n_wavs * n_windows, n_mfcc)\n",
    "# labels: (33408) (n_wavs * n_windows)\n",
    "data = np.array([ all_mfcc[wav_file_idx][:, window_idx] \n",
    "                  for wav_file_idx in range(n_files) \n",
    "                  for window_idx in range(n_windows) ])\n",
    "labels = np.array([ all_vowel_labels[wav_file_idx]\n",
    "                    for wav_file_idx in range(n_files)\n",
    "                    for window_idx in range(n_windows) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purposes - verify that the mfcc have been arranged in order of\n",
    "# wav_idx, win_idx, mel_feature_idx\n",
    "for wav_idx in range(n_files):\n",
    "    for win_idx in range(n_windows):\n",
    "        for m in range(n_mfcc_dummy):\n",
    "            assert data[flat_data_idx(wav_idx, win_idx)][m] == \\\n",
    "                   all_mfcc[wav_idx][m][win_idx]\n",
    "# Verify that the labels are arranged in order of wav_idx, win_idx\n",
    "for wav_idx in range(n_files):\n",
    "    for win_idx in range(n_windows):\n",
    "        assert labels[flat_data_idx(wav_idx, win_idx)] == \\\n",
    "               all_vowel_labels[wav_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into `train` and `test`, and convert to Torch tensors of the correct types. Run **only one of these cells.**\n",
    "\n",
    "First method (**not-recommended**, simple): Random sampling to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (25056, 20) (25056) \n",
    "# X_val, Y_val:     (8352, 20) (8352)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, stratify=labels, random_state=0)\n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "# Used as index, so it is long\n",
    "Y_train = Y_train.long(); Y_val = Y_val.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second method **(recommended)**: 1 person from each wav will be the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (22272, 20) (22272,) \n",
    "# X_val, Y_val:     (11136, 20) (11136,)\n",
    "X_train = []; X_val = []; Y_train = []; Y_val = []\n",
    "for vow_idx in range(n_vowels):\n",
    "    for pit_idx in range(n_pitches):\n",
    "        # Choose the person for this pitch/vowel to be used as test data\n",
    "        test_pid = int(np.random.rand() * 3)\n",
    "        for pid_idx in range(n_people):\n",
    "            wav_idx = flat_ref_idx(vow_idx, pit_idx, pid_idx)\n",
    "            if (pid_idx != test_pid):\n",
    "                for win_idx in range(n_windows):\n",
    "                    X_train.append(data[flat_data_idx(wav_idx, win_idx)])\n",
    "                    Y_train.append(labels[flat_data_idx(wav_idx, win_idx)])\n",
    "            else:\n",
    "                for win_idx in range(n_windows):\n",
    "                    X_val.append(data[flat_data_idx(wav_idx, win_idx)])\n",
    "                    Y_val.append(labels[flat_data_idx(wav_idx, win_idx)])  \n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "# Used as index, so it is long\n",
    "Y_train = Y_train.long(); Y_val = Y_val.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timbre-Encoder - MFCC -> Vowel\n",
    "This takes MFCC (and mel-spectrograms in future?), and tries to identify the vowel spoken.\n",
    "\n",
    "**Results:**\n",
    "```\n",
    "| epochs | n_mfcc | n_hid | n_timb | Val acc |\n",
    "|   5000 |     20 |    12 |      4 | 0.77898 |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hid = 12; n_timb = 4; lr = 0.2; n_epochs = 5000;\n",
    "\n",
    "# Training model \n",
    "model = TimbreEncoder(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "# Define loss \n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# Define optimizer \n",
    "# opt = optim.SGD(model.parameters(), lr=lr)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = model.train_func(X_train, Y_train, X_val, Y_val, model, opt,\n",
    "                        loss_fn, epochs=n_epochs, print_graph=True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreEncoder_{}_{}_{}_{}_{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_mfcc, n_hid, n_timb, loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction (whole dataset) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimbreEncoder(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "# model.to(device)\n",
    "\n",
    "data_tensor, label_tensor = map(torch.tensor, (data, labels))\n",
    "data_tensor = data_tensor.float(); label_tensor = label_tensor.long(); \n",
    "# data_tensor = data_tensor.to(device); label_tensor = label_tensor.to(device)\n",
    "\n",
    "correct = 0; wrong = 0;\n",
    "corrects = np.zeros(n_vowels); wrongs = np.zeros(n_vowels)\n",
    "predictions = np.zeros((n_vowels, n_vowels));\n",
    "for vowel_idx in range(n_vowels):\n",
    "    for pitch_idx in range(n_pitches):\n",
    "        for pid_idx in range(n_people):\n",
    "            wav_idx = flat_ref_idx(vowel_idx, pitch_idx, pid_idx)\n",
    "            for win_idx in range(n_windows):\n",
    "                data_idx = flat_data_idx(wav_idx, win_idx)\n",
    "                label = (label_tensor[data_idx]).item()\n",
    "                pred = (torch.argmax(model(data_tensor[data_idx]))).item()\n",
    "                \n",
    "                predictions[vowel_idx][pred] = predictions[vowel_idx][pred] + 1\n",
    "                if label == pred:\n",
    "                    correct = correct + 1\n",
    "                    corrects[vowel_idx] = corrects[vowel_idx] + 1\n",
    "                else:\n",
    "                    wrong = wrong + 1\n",
    "                    wrongs[vowel_idx] = wrongs[vowel_idx] + 1\n",
    "                    \n",
    "print(\"Total Accuracy: {}\"\n",
    "      .format(correct / (wrong + correct)))\n",
    "for vowel_idx in range(n_vowels):\n",
    "    print(\"Vowel: {}. Accuracy: {}. Most common pred: {}\"\n",
    "          .format(label_to_vowel[vowel_idx],\n",
    "                  corrects[vowel_idx] / (wrongs[vowel_idx] + corrects[vowel_idx]),\n",
    "                  label_to_vowel[np.argmax(predictions[vowel_idx])]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timbre-VAE - MFCC -> MFCC\n",
    "This takes MFCC, reduces dimensionality to a `n_timb` latent space, and attempts to recreate the MFCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hid = 10; n_timb = 4; lr = 1e-3; n_epochs = 10000; batch_size=22272\n",
    "\n",
    "# Training model \n",
    "model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "\n",
    "# Define loss - from pytorch VAE example.\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# Define optimizer \n",
    "# opt = optim.SGD(model.parameters(), lr=lr)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = model.train_func(X_train, X_val, model, opt, loss_fn, batch_size=batch_size,\n",
    "                        epochs=n_epochs, print_graph = True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreVAE_{}_{}_{}_{}_{}_{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_mfcc, n_hid, n_timb, batch_size, loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "data_tensor = torch.tensor(data)\n",
    "data_tensor = data_tensor.float();\n",
    "\n",
    "wav_idx = flat_ref_idx(5, 5, 1)\n",
    "data_idx = flat_data_idx(wav_idx, 30)\n",
    "label = data_tensor[data_idx]\n",
    "pred = model(data_tensor[data_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Choosing `n_hid`, `n_timb`\n",
    "\n",
    "From running this, we find the results:\n",
    "Best `n_hid`: 10, Best `n_timb`: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d296f7d35d4c74acf7f46dab5925ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 6, n_timb: 4', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 6, n_timb: 4, Final val loss: 12.756245229436063, Time taken: 24.036638021469116\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_6_4_12.756245229436063.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637a70bd5180424cb248d25fc7782aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 8, n_timb: 4', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 8, n_timb: 4, Final val loss: 12.756556719198993, Time taken: 23.402870178222656\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_8_4_12.756556719198993.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4631374efbbf4dd393a26627aaf5e0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 8, n_timb: 6', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 8, n_timb: 6, Final val loss: 12.757583793552442, Time taken: 24.187214136123657\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_8_6_12.757583793552442.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1234f6522da7490f8fea6ba030324f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 4, Final val loss: 12.755354256465518, Time taken: 23.62875461578369\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_4_12.755354256465518.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffe0fb4c21e4813a77fe5e903c54b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 6, Final val loss: 12.756701239224139, Time taken: 23.57104468345642\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_6_12.756701239224139.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3ffe45c5104ad68621a431a255b167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 8, Final val loss: 12.75628732264727, Time taken: 23.87569832801819\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_8_12.75628732264727.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfdae241df14f52b49ad43e45a25f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 4, Final val loss: 12.75652725395115, Time taken: 23.377437353134155\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_4_12.75652725395115.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bdd35cf8da4354b09ed4e8ef78ad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 6, Final val loss: 12.756516029094827, Time taken: 24.132755041122437\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_6_12.756516029094827.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80524de2ec654ee5acf77ad615f62c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 8, Final val loss: 12.757957020025144, Time taken: 21.46918272972107\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_8_12.757957020025144.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcd9d9a0a3343c9a7652928ae3ff05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 10, Final val loss: 12.757127783764368, Time taken: 24.31379532814026\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_10_12.757127783764368.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8c7d87690647749f976883fcee2983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 4, Final val loss: 12.75653426948635, Time taken: 23.120083808898926\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_4_12.75653426948635.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549a86014cec4551b236af68296f1858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 6, Final val loss: 12.75579904139727, Time taken: 23.299814462661743\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_6_12.75579904139727.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f86b8c40f744bbb90b1b531ebcb35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 8, Final val loss: 12.757596421515805, Time taken: 24.524491548538208\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_8_12.757596421515805.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277dd35e7e734f41aa39c29bc911df44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 10, Final val loss: 12.756250841864224, Time taken: 24.066789388656616\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_10_12.756250841864224.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddccc7066410466cbdd1f752cbcedcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 12, Final val loss: 12.757847577676007, Time taken: 23.379464149475098\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_12_12.757847577676007.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77a68e0d1ad433f9c96b54812956090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 4, Final val loss: 12.75629153196839, Time taken: 23.141919136047363\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_4_12.75629153196839.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29807120aed403587af8f2212b5cfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 6, Final val loss: 12.75677841011135, Time taken: 23.408835649490356\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_6_12.75677841011135.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbd525a318d4eb3858a38987e7fbc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 8, Final val loss: 12.757703057650861, Time taken: 23.205310583114624\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_8_12.757703057650861.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ad8405d7141fc9240f6a1c2167747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 10, Final val loss: 12.756217167295258, Time taken: 18.973145723342896\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_10_12.756217167295258.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb355397ada040c6a7f8bf467ec87881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 12, Final val loss: 12.756746138649426, Time taken: 18.91083335876465\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_12_12.756746138649426.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6f9708fc6b43bea0f2924e35c104b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 14', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 14, Final val loss: 12.757397180316092, Time taken: 19.097800970077515\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_14_12.757397180316092.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b581abac8004f38b745c698ece54d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 4, Final val loss: 12.756252244971265, Time taken: 18.403200149536133\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_4_12.756252244971265.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5c5069a36242f096ed402dba273109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 6, Final val loss: 12.755765366828305, Time taken: 20.536202669143677\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_6_12.755765366828305.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8c0252adfb4320b68b49d0684bb304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 8, Final val loss: 12.756218570402298, Time taken: 18.79640245437622\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_8_12.756218570402298.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785fe56719e345f5b9911198c0e7a7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 10, Final val loss: 12.757287737966953, Time taken: 19.083486557006836\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_10_12.757287737966953.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69588899cbf04340b123a9f65a4a3c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 12, Final val loss: 12.757718491828305, Time taken: 19.04806160926819\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_12_12.757718491828305.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39afd0e0656a482c94401b5f7a4a7caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 14', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 14, Final val loss: 12.75823904454023, Time taken: 19.794686317443848\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_14_12.75823904454023.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3e05805e848aa9c6decf75e2f9a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 16', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 16, Final val loss: 12.756389749461206, Time taken: 18.548084497451782\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_16_12.756389749461206.pt\n"
     ]
    }
   ],
   "source": [
    "# Define loss - from pytorch VAE example.\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "n_hid_candidates = [6, 8, 10, 12, 14, 16, 18]\n",
    "n_timb_candidates = [4, 6, 8, 10, 12, 14, 16]\n",
    "lr = 1e-3; n_epochs = 2500; batch_size=22272\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "losses = []\n",
    "for hid_idx in range(len(n_hid_candidates)):\n",
    "    for timb_idx in range(len(n_timb_candidates)):\n",
    "        if (hid_idx < timb_idx): continue\n",
    "        n_hid = n_hid_candidates[hid_idx]\n",
    "        n_timb = n_timb_candidates[timb_idx]\n",
    "        \n",
    "        # Training model \n",
    "        model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "\n",
    "        X_train = X_train.to(device)\n",
    "        X_val = X_val.to(device)\n",
    "        model.to(device) \n",
    "        # opt = optim.SGD(model.parameters(), lr=lr)\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Fit the model\n",
    "        tic = time.time()\n",
    "        loss, val_loss = model.train_func(X_train, X_val, model, opt, loss_fn, batch_size=batch_size,\n",
    "                                epochs=n_epochs, print_graph=False, desc=\"n_hid: {}, n_timb: {}\".format(n_hid, n_timb))\n",
    "        toc = time.time()\n",
    "        print('n_hid: {}, n_timb: {}, Final val loss: {}, Time taken: {}'.format(n_hid, n_timb, val_loss, toc - tic))\n",
    "        model_path = os.path.join(\"model_data\", \"TimbreVAE_n_hid_n_timb_experiment_{}_{}_{}.pt\"\n",
    "                                  .format(n_hid, n_timb, val_loss))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"Model saved at {}\".format(model_path)) \n",
    "        losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_hid: 10, Best n_timb: 4\n",
      "[[12.756245229436063 list([6, 4])]\n",
      " [12.756556719198993 list([8, 4])]\n",
      " [12.757583793552442 list([8, 6])]\n",
      " [12.755354256465518 list([10, 4])]\n",
      " [12.756701239224139 list([10, 6])]\n",
      " [12.75628732264727 list([10, 8])]\n",
      " [12.75652725395115 list([12, 4])]\n",
      " [12.756516029094827 list([12, 6])]\n",
      " [12.757957020025144 list([12, 8])]\n",
      " [12.757127783764368 list([12, 10])]\n",
      " [12.75653426948635 list([14, 4])]\n",
      " [12.75579904139727 list([14, 6])]\n",
      " [12.757596421515805 list([14, 8])]\n",
      " [12.756250841864224 list([14, 10])]\n",
      " [12.757847577676007 list([14, 12])]\n",
      " [12.75629153196839 list([16, 4])]\n",
      " [12.75677841011135 list([16, 6])]\n",
      " [12.757703057650861 list([16, 8])]\n",
      " [12.756217167295258 list([16, 10])]\n",
      " [12.756746138649426 list([16, 12])]\n",
      " [12.757397180316092 list([16, 14])]\n",
      " [12.756252244971265 list([18, 4])]\n",
      " [12.755765366828305 list([18, 6])]\n",
      " [12.756218570402298 list([18, 8])]\n",
      " [12.757287737966953 list([18, 10])]\n",
      " [12.757718491828305 list([18, 12])]\n",
      " [12.75823904454023 list([18, 14])]\n",
      " [12.756389749461206 list([18, 16])]]\n"
     ]
    }
   ],
   "source": [
    "indices = [ [n_hid_candidates[hid_idx], n_timb_candidates[timb_idx]] \n",
    "            for hid_idx in range(len(n_hid_candidates))\n",
    "            for timb_idx in range(len(n_timb_candidates)) if (hid_idx >= timb_idx) ]\n",
    "best_hid, best_timb = indices[np.argmin(np.array(losses))]\n",
    "print(\"Best n_hid: {}, Best n_timb: {}\".format(best_hid, best_timb))\n",
    "print(np.array(list(zip(losses, indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
