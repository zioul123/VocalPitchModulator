{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timbre Encoder\n",
    "This is the notebook used to train the Vocal Pitch Modulator.\n",
    "\n",
    "This notebook makes use of the data to train our timbre encoder.\n",
    "\n",
    "There are two models here, the first is a vowel classifier, that takes in an MFCC and outputs a vowel, and the second is a VAE that takes in an MFCC, reduces its dimensionality, and attempts to reconstruct the provided MFCC.\n",
    "\n",
    "## Global variables/Imports\n",
    "Run these cells before running either of the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%aimport VPM\n",
    "from VPM import *\n",
    "%aimport Utils\n",
    "from Utils import *\n",
    "%aimport ANN\n",
    "from ANN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that should not change without the dataset being changed\n",
    "n_pitches = 16\n",
    "n_vowels = 12\n",
    "n_people = 3\n",
    "\n",
    "# These dictionaries are more for reference than anything\n",
    "label_to_vowel = { 0: \"bed\",  1: \"bird\",   2: \"boat\",  3: \"book\", \n",
    "                   4: \"cat\",  5: \"dog\",    6: \"feet\",  7: \"law\",  \n",
    "                   8: \"moo\",  9: \"nut\",   10: \"pig\",  11: \"say\" }\n",
    "\n",
    "vowel_to_label = { \"bed\": 0,  \"bird\": 1,  \"boat\":  2, \"book\":  3,\n",
    "                   \"cat\": 4,  \"dog\":  5,  \"feet\":  6, \"law\":   7,\n",
    "                   \"moo\": 8,  \"nut\":  9,  \"pig\":  10, \"say\":  11}\n",
    "\n",
    "noteidx_to_pitch = {  0: \"A2\",   1: \"Bb2\",  2: \"B2\",   3: \"C3\",\n",
    "                      4: \"Db3\",  5: \"D3\",   6: \"Eb3\",  7: \"E3\", \n",
    "                      8: \"F3\",   9: \"Gb3\", 10: \"G3\",  11: \"Ab3\",\n",
    "                     12: \"A3\",  13: \"Bb3\", 14: \"B3\",  15: \"C4\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Used to tune the data generation and ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "n_mfcc = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "This is all the code that was explained in the Data Walkthrough. It generates data structures to hold all wav file data, spectrograms, mel spectra and MFCC data for all wav files.\n",
    "\n",
    "For diagram-visualization of the data set, refer to the [readme](https://github.com/zioul123/VocalPitchModulator/blob/master/README.md).\n",
    "\n",
    "For the classifier, MFCC are normalized by row to `[-1, 1]`, as the classifer can learn it better.\n",
    "For the VAE, MFCC are normalized by row to `[0, 1]`, to allow the VAE to output it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# File reference lists\n",
    "data_ref_list = create_data_ref_list(os.path.join(\"Data\", 'dataset_files.csv'),\n",
    "                            n_pitches, n_vowels, n_people)\n",
    "# flat_data_ref_list[flat_ref_idx(vowel, pitch, person)]\n",
    "flat_data_ref_list = flatten_3d_array(data_ref_list, \n",
    "                                      n_vowels, n_pitches, n_people)\n",
    "\n",
    "# File reference list accessors\n",
    "# Returns a flat_ref_idx, given a vowel, pitch, person\n",
    "flat_ref_idx = lambda vowel, pitch, person: flat_3d_array_idx(\n",
    "    vowel, pitch, person, n_vowels, n_pitches, n_people)\n",
    "# Returns vowel, pitch, person, given a flat_ref_idx\n",
    "nd_ref_idx = lambda idx: nd_array_idx(idx, n_vowels, n_pitches, n_people)\n",
    "\n",
    "# Data-label pairs for pitch-shift training - not used here\n",
    "# data_label_pairs, _ = create_data_label_pairs(n_pitches)\n",
    "\n",
    "# wav, spectrogram, mels, mfcc for each file in flat_data_ref_list\n",
    "# wav_data:     (576, ~29400)  (n_wavs, n_samples)\n",
    "# spectrograms: (576, 513, 58) (n_wavs, n_freq_bins, n_windows)\n",
    "# mels:         (576, 128, 58) (n_wavs, n_mels, n_windows)\n",
    "# mfccs:        (576, 20, 58)  (n_wavs, n_mfcc, n_windows)\n",
    "all_wav_data = load_wav_files(os.path.join(\"Data\", \"dataset\"), \n",
    "                              flat_data_ref_list)\n",
    "all_spectrograms = np.array([ stft(waveform, plot=False) \n",
    "                              for waveform in all_wav_data ])\n",
    "all_mels, all_mfcc = map(np.array, map(list, zip(*\n",
    "                         [ ffts_to_mel(ffts, n_mels = n_mels, n_mfcc = n_mfcc) \n",
    "                           for ffts in all_spectrograms ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-Label Structuring\n",
    "This puts together the actual data-label pairs to be fed into the ANN.\n",
    "\n",
    "Generate `data` and `labels` from `all_mfcc` and using `nd_ref_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_files, n_mfcc_dummy, n_windows = all_mfcc.shape\n",
    "\n",
    "# vowel_labels: (576) (n_wavs)\n",
    "all_vowel_labels, _, _ = map(np.array, map(list, zip(*\n",
    "                         [ nd_ref_idx(idx) \n",
    "                           for idx in range(len(flat_data_ref_list)) ])))\n",
    "\n",
    "# Returns a flat 2d idx, given a wavfile index and a window index\n",
    "flat_data_idx = lambda wav_idx, win_idx: flat_2d_array_idx(\n",
    "    wav_idx, win_idx, n_files, n_windows)\n",
    "\n",
    "# data:   (33408, 20) (n_wavs * n_windows, n_mfcc)\n",
    "# labels: (33408) (n_wavs * n_windows)\n",
    "data_prenorm = np.array([ all_mfcc[wav_file_idx][:, window_idx] \n",
    "                          for wav_file_idx in range(n_files) \n",
    "                          for window_idx in range(n_windows) ])\n",
    "labels = np.array([ all_vowel_labels[wav_file_idx]\n",
    "                    for wav_file_idx in range(n_files)\n",
    "                    for window_idx in range(n_windows) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment one of these lines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize each mfcc (i.e. rows of the all_mfcc array) to [-1,1] - USE FOR THE CLASSIFER (TimbreEncoder)\n",
    "# data = normalize_rows(data_prenorm, NormMode.REAL_TO_NEG_ONE_ONE)\n",
    "#Normalize each mfcc (i.e. rows of the all_mfcc array) to [0,1] - USE FOR THE VAE (TimbreVAE)\n",
    "data, scale = normalize_rows(data_prenorm, NormMode.REAL_TO_ZERO_ONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can retrieve the mfccs as follows:\n",
    "# wav_idx = 0\n",
    "# print((data[wav_idx] * 2 - 1) / scale[wav_idx])\n",
    "# print(data_prenorm[wav_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These assertions no longer work after normalization, you can do them if you do not normalize the data array.\n",
    "# # For testing purposes - verify that the mfcc have been arranged in order of\n",
    "# # wav_idx, win_idx, mel_feature_idx\n",
    "# for wav_idx in range(n_files):\n",
    "#     for win_idx in range(n_windows):\n",
    "#         for m in range(n_mfcc_dummy):\n",
    "#             assert data[flat_data_idx(wav_idx, win_idx)][m] == \\\n",
    "#                    all_mfcc[wav_idx][m][win_idx]\n",
    "# # Verify that the labels are arranged in order of wav_idx, win_idx\n",
    "# for wav_idx in range(n_files):\n",
    "#     for win_idx in range(n_windows):\n",
    "#         assert labels[flat_data_idx(wav_idx, win_idx)] == \\\n",
    "#                all_vowel_labels[wav_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data into `train` and `test`, and convert to Torch tensors of the correct types. Run **only one of these cells.**\n",
    "\n",
    "First method (**not-recommended**, simple): Random sampling to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (25056, 20) (25056) \n",
    "# X_val, Y_val:     (8352, 20) (8352)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, stratify=labels, random_state=0)\n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "# Used as index, so it is long\n",
    "Y_train = Y_train.long(); Y_val = Y_val.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second method **(recommended)**: 1 person from each wav will be the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, Y_train: (22272, 20) (22272,) \n",
    "# X_val, Y_val:     (11136, 20) (11136,)\n",
    "X_train = []; X_val = []; Y_train = []; Y_val = []\n",
    "for vow_idx in range(n_vowels):\n",
    "    for pit_idx in range(n_pitches):\n",
    "        # Choose the person for this pitch/vowel to be used as test data\n",
    "        test_pid = int(np.random.rand() * 3)\n",
    "        for pid_idx in range(n_people):\n",
    "            wav_idx = flat_ref_idx(vow_idx, pit_idx, pid_idx)\n",
    "            if (pid_idx != test_pid):\n",
    "                for win_idx in range(n_windows):\n",
    "                    X_train.append(data[flat_data_idx(wav_idx, win_idx)])\n",
    "                    Y_train.append(labels[flat_data_idx(wav_idx, win_idx)])\n",
    "            else:\n",
    "                for win_idx in range(n_windows):\n",
    "                    X_val.append(data[flat_data_idx(wav_idx, win_idx)])\n",
    "                    Y_val.append(labels[flat_data_idx(wav_idx, win_idx)])  \n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "# Default tensor is float\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "# Used as index, so it is long\n",
    "Y_train = Y_train.long(); Y_val = Y_val.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timbre-Encoder - MFCC -> Vowel\n",
    "This takes MFCC (and mel-spectrograms in future?), and tries to identify the vowel spoken.\n",
    "\n",
    "**Results:**\n",
    "```\n",
    "| epochs | n_mfcc | n_hid | n_timb | Val acc |\n",
    "|   5000 |     20 |    12 |      4 | 0.77898 |\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hid = 12; n_timb = 4; lr = 0.2; n_epochs = 5000;\n",
    "\n",
    "# Training model \n",
    "model = TimbreEncoder(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "# Define loss \n",
    "loss_fn = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# Define optimizer \n",
    "# opt = optim.SGD(model.parameters(), lr=lr)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = model.train_func(X_train, Y_train, X_val, Y_val, model, opt,\n",
    "                        loss_fn, epochs=n_epochs, print_graph=True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreEncoder_{}_{}_{}_{}_{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_mfcc, n_hid, n_timb, loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction (whole dataset) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimbreEncoder(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "# model.to(device)\n",
    "\n",
    "data_tensor, label_tensor = map(torch.tensor, (data, labels))\n",
    "data_tensor = data_tensor.float(); label_tensor = label_tensor.long(); \n",
    "# data_tensor = data_tensor.to(device); label_tensor = label_tensor.to(device)\n",
    "\n",
    "correct = 0; wrong = 0;\n",
    "corrects = np.zeros(n_vowels); wrongs = np.zeros(n_vowels)\n",
    "predictions = np.zeros((n_vowels, n_vowels));\n",
    "for vowel_idx in range(n_vowels):\n",
    "    for pitch_idx in range(n_pitches):\n",
    "        for pid_idx in range(n_people):\n",
    "            wav_idx = flat_ref_idx(vowel_idx, pitch_idx, pid_idx)\n",
    "            for win_idx in range(n_windows):\n",
    "                data_idx = flat_data_idx(wav_idx, win_idx)\n",
    "                label = (label_tensor[data_idx]).item()\n",
    "                pred = (torch.argmax(model(data_tensor[data_idx]))).item()\n",
    "                \n",
    "                predictions[vowel_idx][pred] = predictions[vowel_idx][pred] + 1\n",
    "                if label == pred:\n",
    "                    correct = correct + 1\n",
    "                    corrects[vowel_idx] = corrects[vowel_idx] + 1\n",
    "                else:\n",
    "                    wrong = wrong + 1\n",
    "                    wrongs[vowel_idx] = wrongs[vowel_idx] + 1\n",
    "                    \n",
    "print(\"Total Accuracy: {}\"\n",
    "      .format(correct / (wrong + correct)))\n",
    "for vowel_idx in range(n_vowels):\n",
    "    print(\"Vowel: {}. Accuracy: {}. Most common pred: {}\"\n",
    "          .format(label_to_vowel[vowel_idx],\n",
    "                  corrects[vowel_idx] / (wrongs[vowel_idx] + corrects[vowel_idx]),\n",
    "                  label_to_vowel[np.argmax(predictions[vowel_idx])]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timbre-VAE - MFCC -> MFCC\n",
    "This takes MFCC, reduces dimensionality to a `n_timb` latent space, and attempts to recreate the MFCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hid = 10; n_timb = 4; lr = 1e-3; n_epochs = 10000; batch_size=22272\n",
    "\n",
    "# Training model \n",
    "model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "\n",
    "# Define loss - from pytorch VAE example.\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237cf6804b944347ad29bb7178b3ff2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=10000.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss before/after: 14.152216067259339, 12.750126279633621\n",
      "Validation Loss before/after: 14.134939610272989, 12.748387830010776\n",
      "Final loss: (12.750126279633621, 12.748378008261493)\n",
      "Time taken: 95.22735357284546\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# Define optimizer \n",
    "# opt = optim.SGD(model.parameters(), lr=lr)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = model.train_func(X_train, X_val, model, opt, loss_fn, batch_size=batch_size,\n",
    "                        epochs=n_epochs, print_graph = True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "en1.weight \t torch.Size([10, 20])\n",
      "en1.bias \t torch.Size([10])\n",
      "en_mu.weight \t torch.Size([4, 10])\n",
      "en_mu.bias \t torch.Size([4])\n",
      "en_std.weight \t torch.Size([4, 10])\n",
      "en_std.bias \t torch.Size([4])\n",
      "de1.weight \t torch.Size([10, 4])\n",
      "de1.bias \t torch.Size([10])\n",
      "de2.weight \t torch.Size([20, 10])\n",
      "de2.bias \t torch.Size([20])\n",
      "Model saved at model_data\\TimbreVAE_lr0.001_n_epochs10000_n_mfcc20_n_hid10_n_timb4_(12.750126279633621, 12.748378008261493).pt\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreVAE_lr{}_n_epochs{}_n_mfcc{}_n_hid{}_n_timb{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_mfcc, n_hid, n_timb, loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-254.13701    241.4798       1.799128    38.29932      6.9060698\n",
      "  -45.34479    -47.308533    10.779621    44.913612   -30.398193\n",
      "  -24.722454    -3.5992258  -18.537493   -16.140165     8.499067\n",
      "  -14.776051    17.55851     16.277525   -19.305908    -4.0421457]\n",
      "[-254.13702    241.47981      1.7991194   38.29932      6.9060593\n",
      "  -45.34479    -47.30854     10.779635    44.91362    -30.398193\n",
      "  -24.722454    -3.5992284  -18.537493   -16.14016      8.499054\n",
      "  -14.776056    17.558506    16.27752    -19.305904    -4.0421457]\n",
      "\n",
      "[-242.87985     226.72884     -34.92098      33.24506       5.3138595\n",
      "   -3.28491     -36.46167      -4.0434184    -0.23548687  -31.024675\n",
      "  -14.2286415    -7.1847577    -9.985894    -13.498308     -4.9755044\n",
      "   -8.887683     -1.1321728    -3.7054567    -5.773321    -12.994539  ]\n"
     ]
    }
   ],
   "source": [
    "model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "data_tensor = torch.tensor(data)\n",
    "data_tensor = data_tensor.float();\n",
    "\n",
    "wav_idx = flat_ref_idx(5, 5, 1)\n",
    "data_idx = flat_data_idx(wav_idx, 30)\n",
    "label = data_tensor[data_idx]\n",
    "pred = model(data_tensor[data_idx])\n",
    "\n",
    "\n",
    "# Rescaling the data array is the same as prenorm\n",
    "print((data[data_idx] * 2 - 1) / scale[data_idx]) \n",
    "print(data_prenorm[data_idx])\n",
    "# We see the prediction is roughly on the same order of magnitude as prenorm -> Autoencoder is successful\n",
    "np.set_printoptions(suppress=True)\n",
    "print((pred[0].detach().numpy() * 2 - 1) / scale[data_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Choosing `n_hid`, `n_timb`\n",
    "\n",
    "From running this, we find the results:\n",
    "Best `n_hid`: 10, Best `n_timb`: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33637ee9d8b4edea2e61ae9e2763b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 6, n_timb: 4', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 6, n_timb: 4, Final val loss: 12.749682897808908, Time taken: 19.85393738746643\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_6_4_12.749682897808908.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9318c65f8234847b34d906f2f219f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 8, n_timb: 4', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 8, n_timb: 4, Final val loss: 12.75012487652658, Time taken: 18.63666820526123\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_8_4_12.75012487652658.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43643aea101f44d28c1f10e0693748e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 8, n_timb: 6', max=2500.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 8, n_timb: 6, Final val loss: 12.751053733387213, Time taken: 18.740490198135376\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_8_6_12.751053733387213.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d707e6c184c4350b879afead7090139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 4, Final val loss: 12.748828405621408, Time taken: 19.146907567977905\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_10_4_12.748828405621408.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9275a811a0c84a4bb70172c13d919e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 6, Final val loss: 12.750084186422415, Time taken: 21.91523814201355\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_10_6_12.750084186422415.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a14e60494394a8c9bf329e00bea1c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 8, Final val loss: 12.749841448904453, Time taken: 21.23058032989502\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_10_8_12.749841448904453.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71885f938484c339aef1b39d39d9881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 4, Final val loss: 12.749834433369253, Time taken: 22.319886922836304\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_12_4_12.749834433369253.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cefbf8f4a34b0c8616b3e6bbd28ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 6, Final val loss: 12.749956503681753, Time taken: 20.11144781112671\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_12_6_12.749956503681753.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff1e6c70b5f4ca096fbfeb3e5babc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 8, Final val loss: 12.751195447198276, Time taken: 19.888671875\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_12_8_12.751195447198276.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c352e47033406c876efe270eed5c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 10, Final val loss: 12.75073101876796, Time taken: 24.067354202270508\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_12_10_12.75073101876796.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e8698ee74b436cbad29697f74f8c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 4, Final val loss: 12.74981478987069, Time taken: 21.181243181228638\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_14_4_12.74981478987069.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faabbe4b39cd44299294ca5ce0a373d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 6, Final val loss: 12.749259159482758, Time taken: 23.346937656402588\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_14_6_12.749259159482758.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c21754a69642f4a27acd08a7304625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 8, Final val loss: 12.750991996677442, Time taken: 22.373285055160522\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_14_8_12.750991996677442.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8401cb452354d94b1c40e2e14e1951b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 10, Final val loss: 12.74981058054957, Time taken: 22.019381999969482\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_14_10_12.74981058054957.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5667c2e3614965984e785232cf14db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 12, Final val loss: 12.751274021192529, Time taken: 21.046178579330444\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_14_12_12.751274021192529.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff20484936354db4aabbce6005477cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 4, Final val loss: 12.749685704022989, Time taken: 20.65230703353882\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_16_4_12.749685704022989.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab65d3de57f4354bcbe185788d0f881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 6, Final val loss: 12.75018521012931, Time taken: 20.2109158039093\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_16_6_12.75018521012931.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f008ddd73b6540e78de71b18e0f9c51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 8, Final val loss: 12.750955515894397, Time taken: 21.95601201057434\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_16_8_12.750955515894397.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8f7210994243c7b729fe4573473a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 10, Final val loss: 12.74969552577227, Time taken: 23.390918016433716\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_16_10_12.74969552577227.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091bd266a2f54ddbad69432b8031a3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 12, Final val loss: 12.75029886179957, Time taken: 21.056029796600342\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_16_12_12.75029886179957.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365e20f3de7348a4a7bbf935f0f03ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 14', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 14, Final val loss: 12.750889569863507, Time taken: 21.883458375930786\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_16_14_12.750889569863507.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec017cb0537948308e8b462c02d429a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 4', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 4, Final val loss: 12.749797952586206, Time taken: 19.31596040725708\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_18_4_12.749797952586206.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df39cfa03b534f2880e05c485bed6f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 6', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 6, Final val loss: 12.749281609195402, Time taken: 19.58797001838684\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_18_6_12.749281609195402.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e5bbe6c14a4c3f84d7991fd89206ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 8', max=2500.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 8, Final val loss: 12.749684300915948, Time taken: 19.6473069190979\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_18_8_12.749684300915948.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c96fdb8e5b4134a6c295ae8e83b18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 10', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 10, Final val loss: 12.750893779184626, Time taken: 21.02516531944275\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_18_10_12.750893779184626.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e6c701da9846649868768dd67a6099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 12', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 12, Final val loss: 12.751188431663074, Time taken: 20.871736764907837\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_18_12_12.751188431663074.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0554526e854ba3a284b2e729b8066c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 14', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 14, Final val loss: 12.751575689206177, Time taken: 20.64602518081665\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_18_14_12.751575689206177.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8d19770b0b4ea6b5fa88bf467f820e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 16', max=2500.0, style=ProgressStyle(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 16, Final val loss: 12.74988073590158, Time taken: 20.266278743743896\n",
      "Model saved at model_data\\TimbreVAE_EXPT_n_hid_n_timb_experiment_18_16_12.74988073590158.pt\n"
     ]
    }
   ],
   "source": [
    "# Define loss - from pytorch VAE example.\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "n_hid_candidates = [6, 8, 10, 12, 14, 16, 18]\n",
    "n_timb_candidates = [4, 6, 8, 10, 12, 14, 16]\n",
    "lr = 1e-3; n_epochs = 2500; batch_size=22272\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "losses = []\n",
    "for hid_idx in range(len(n_hid_candidates)):\n",
    "    for timb_idx in range(len(n_timb_candidates)):\n",
    "        if (hid_idx < timb_idx): continue\n",
    "        n_hid = n_hid_candidates[hid_idx]\n",
    "        n_timb = n_timb_candidates[timb_idx]\n",
    "        \n",
    "        # Training model \n",
    "        model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "\n",
    "        X_train = X_train.to(device)\n",
    "        X_val = X_val.to(device)\n",
    "        model.to(device) \n",
    "        # opt = optim.SGD(model.parameters(), lr=lr)\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Fit the model\n",
    "        tic = time.time()\n",
    "        loss, val_loss = model.train_func(X_train, X_val, model, opt, loss_fn, batch_size=batch_size,\n",
    "                                epochs=n_epochs, print_graph=False, desc=\"n_hid: {}, n_timb: {}\".format(n_hid, n_timb))\n",
    "        toc = time.time()\n",
    "        print('n_hid: {}, n_timb: {}, Final val loss: {}, Time taken: {}'.format(n_hid, n_timb, val_loss, toc - tic))\n",
    "        model_path = os.path.join(\"model_data\", \"TimbreVAE_EXPT_n_hid_n_timb_experiment_{}_{}_{}.pt\"\n",
    "                                  .format(n_hid, n_timb, val_loss))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"Model saved at {}\".format(model_path)) \n",
    "        losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_hid: 10, Best n_timb: 4\n",
      "[[12.749682897808908 list([6, 4])]\n",
      " [12.75012487652658 list([8, 4])]\n",
      " [12.751053733387213 list([8, 6])]\n",
      " [12.748828405621408 list([10, 4])]\n",
      " [12.750084186422415 list([10, 6])]\n",
      " [12.749841448904453 list([10, 8])]\n",
      " [12.749834433369253 list([12, 4])]\n",
      " [12.749956503681753 list([12, 6])]\n",
      " [12.751195447198276 list([12, 8])]\n",
      " [12.75073101876796 list([12, 10])]\n",
      " [12.74981478987069 list([14, 4])]\n",
      " [12.749259159482758 list([14, 6])]\n",
      " [12.750991996677442 list([14, 8])]\n",
      " [12.74981058054957 list([14, 10])]\n",
      " [12.751274021192529 list([14, 12])]\n",
      " [12.749685704022989 list([16, 4])]\n",
      " [12.75018521012931 list([16, 6])]\n",
      " [12.750955515894397 list([16, 8])]\n",
      " [12.74969552577227 list([16, 10])]\n",
      " [12.75029886179957 list([16, 12])]\n",
      " [12.750889569863507 list([16, 14])]\n",
      " [12.749797952586206 list([18, 4])]\n",
      " [12.749281609195402 list([18, 6])]\n",
      " [12.749684300915948 list([18, 8])]\n",
      " [12.750893779184626 list([18, 10])]\n",
      " [12.751188431663074 list([18, 12])]\n",
      " [12.751575689206177 list([18, 14])]\n",
      " [12.74988073590158 list([18, 16])]]\n"
     ]
    }
   ],
   "source": [
    "indices = [ [n_hid_candidates[hid_idx], n_timb_candidates[timb_idx]] \n",
    "            for hid_idx in range(len(n_hid_candidates))\n",
    "            for timb_idx in range(len(n_timb_candidates)) if (hid_idx >= timb_idx) ]\n",
    "best_hid, best_timb = indices[np.argmin(np.array(losses))]\n",
    "print(\"Best n_hid: {}, Best n_timb: {}\".format(best_hid, best_timb))\n",
    "print(np.array(list(zip(losses, indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
