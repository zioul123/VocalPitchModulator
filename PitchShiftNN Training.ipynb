{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PitchShiftNN Training\n",
    "This is the notebook used to train the Vocal Pitch Modulator.\n",
    "\n",
    "## Global variables/Imports\n",
    "Run these cells before running either of the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import scipy.io as sio\n",
    "from scipy.io import wavfile\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "%aimport VPM\n",
    "from VPM import *\n",
    "%aimport Utils\n",
    "from Utils import *\n",
    "%aimport PitchShiftNN\n",
    "from PitchShiftNN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants that should not change without the dataset being changed\n",
    "n_pitches = 16\n",
    "n_vowels = 12\n",
    "n_people = 3\n",
    "\n",
    "# These dictionaries are more for reference than anything\n",
    "label_to_vowel = { 0: \"bed\",  1: \"bird\",   2: \"boat\",  3: \"book\", \n",
    "                   4: \"cat\",  5: \"dog\",    6: \"feet\",  7: \"law\",  \n",
    "                   8: \"moo\",  9: \"nut\",   10: \"pig\",  11: \"say\" }\n",
    "\n",
    "vowel_to_label = { \"bed\": 0,  \"bird\": 1,  \"boat\":  2, \"book\":  3,\n",
    "                   \"cat\": 4,  \"dog\":  5,  \"feet\":  6, \"law\":   7,\n",
    "                   \"moo\": 8,  \"nut\":  9,  \"pig\":  10, \"say\":  11}\n",
    "\n",
    "noteidx_to_pitch = {  0: \"A2\",   1: \"Bb2\",  2: \"B2\",   3: \"C3\",\n",
    "                      4: \"Db3\",  5: \"D3\",   6: \"Eb3\",  7: \"E3\", \n",
    "                      8: \"F3\",   9: \"Gb3\", 10: \"G3\",  11: \"Ab3\",\n",
    "                     12: \"A3\",  13: \"Bb3\", 14: \"B3\",  15: \"C4\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "This is copied directly from `Data Processing for Training PitchShiftNN.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# e.g. data_list[vowel_to_label[\"dog\"]][5][1]\n",
    "data_ref_list = create_data_ref_list(os.path.join(\"Data\", 'dataset_files.csv'),\n",
    "                                     n_pitches, n_vowels, n_people)\n",
    "\n",
    "# e.g. flat_data_ref_list[flat_ref_idx(3, 1, 2)]\n",
    "flat_data_ref_list = flatten_3d_array(data_ref_list, \n",
    "                                      n_vowels, n_pitches, n_people)\n",
    "\n",
    "# Returns a flat_ref_idx, given a vowel, pitch, person\n",
    "flat_ref_idx = lambda vowel, pitch, person: flat_3d_array_idx(\n",
    "    vowel, pitch, person, n_vowels, n_pitches, n_people)\n",
    "\n",
    "# Returns vowel, pitch, person, given a flat_ref_idx\n",
    "nd_ref_idx = lambda idx: nd_array_idx(idx, n_vowels, n_pitches, n_people)\n",
    "\n",
    "data_label_pairs, data_label_pairs_dict = create_data_label_pairs(n_pitches)\n",
    "\n",
    "all_wav_data = load_wav_files(os.path.join(\"Data\", \"dataset\"), \n",
    "                              flat_data_ref_list)\n",
    "all_spectrograms = np.array([ stft(waveform, plot=False) for waveform in all_wav_data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22272, 513)\n",
      "(22272, 513)\n",
      "(11136, 513)\n",
      "(11136, 513)\n",
      "(22272, 513)\n",
      "(11136, 513)\n",
      "(22272, 1026)\n",
      "(11136, 1026)\n"
     ]
    }
   ],
   "source": [
    "# EDIT THE SHIFT AMOUNT PARAMETER HERE\n",
    "\n",
    "shift_amt = 0\n",
    "pairs = data_label_pairs_dict[shift_amt]\n",
    "\n",
    "# X_train_base, Y_train: (_,513), (_,513)\n",
    "# X_val_base, Y_val:     (_,513), (_,513)\n",
    "\n",
    "X_train_base = []; X_val_base = []; Y_train = []; Y_val = []\n",
    "\n",
    "for vow_idx in range(n_vowels):\n",
    "    for pit_idx in range(n_pitches):\n",
    "        \n",
    "        # If the pair is valid, then proceed.\n",
    "        if [shift_amt, pit_idx, pit_idx + shift_amt] in pairs:\n",
    "        \n",
    "            # Choose the person for this pitch/vowel to be used as test data.\n",
    "            test_pid = int(np.random.rand() * 3)\n",
    "\n",
    "            for pid_idx in range(n_people):\n",
    "                wav_idx = flat_ref_idx(vow_idx, pit_idx, pid_idx)\n",
    "                wav_idx_shifted = flat_ref_idx(vow_idx, pit_idx + shift_amt, pid_idx)\n",
    "\n",
    "                if (pid_idx != test_pid):\n",
    "                    X_train_base.extend(all_spectrograms[wav_idx].T)\n",
    "                    Y_train.extend(all_spectrograms[wav_idx_shifted].T)\n",
    "                else:\n",
    "                    X_val_base.extend(all_spectrograms[wav_idx].T)\n",
    "                    Y_val.extend(all_spectrograms[wav_idx_shifted].T)\n",
    "\n",
    "X_train_base = np.array(X_train_base); Y_train = np.array(Y_train); X_val_base = np.array(X_val_base); Y_val = np.array(Y_val); \n",
    "                    \n",
    "print(X_train_base.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_val_base.shape)\n",
    "print(Y_val.shape)\n",
    "\n",
    "X_train_shifted = np.array([ simple_fft_pitch_shift(x, shift_amt) for x in X_train_base ])\n",
    "X_val_shifted = np.array([ simple_fft_pitch_shift(x, shift_amt) for x in X_val_base ])\n",
    "\n",
    "print(X_train_shifted.shape)\n",
    "print(X_val_shifted.shape)\n",
    "\n",
    "# X_train, X_val: (_,1026 = 513*2), (_,1026 = 513*2)\n",
    "\n",
    "X_train = np.hstack((X_train_base,X_train_shifted))\n",
    "X_val = np.hstack((X_val_base,X_val_shifted))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "X_train = np.absolute(X_train)\n",
    "X_val = np.absolute(X_val)\n",
    "Y_train = np.absolute(Y_train)\n",
    "Y_val = np.absolute(Y_val)\n",
    "\n",
    "# Turn into PyTorch tensor format\n",
    "\n",
    "X_train, Y_train, X_val, Y_val = map(torch.tensor, (X_train, Y_train, X_val, Y_val))\n",
    "X_train = X_train.float(); X_val = X_val.float()\n",
    "Y_train = Y_train.float(); Y_val = Y_val.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PitchShiftNN\n",
    "\n",
    "Input: `(original window, manual pitch shifted window)`\n",
    "\n",
    "Output: `(natural pitch shifted window)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 1026; n_hid = 513; n_output = 513; \n",
    "n_epochs = 5000; lr = 0.2;\n",
    "\n",
    "# Define model\n",
    "model = PitchShiftNN(n_input = n_input, n_hid = n_hid, n_output = n_output)\n",
    "\n",
    "# Define loss \n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22272, 1026])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15e53d785df4d8893d4c7bb5bbdd0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training', max=5000.0, style=ProgressStyle(description_wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss before/after: 52.800331115722656, 52.11915588378906\n",
      "Training accuracy before/after: 0, 0\n",
      "Validation accuracy before/after: 0, 0\n",
      "Final loss: 52.11915588378906\n",
      "Time taken: 973.202544927597\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "Y_train = Y_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "Y_val = Y_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "\n",
    "# Define optimizer \n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "# opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = model.train_func(X_train, Y_train, X_val, Y_val, model, opt,\n",
    "                        loss_fn, epochs=n_epochs, print_graph=True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreEncoder_{}_{}_{}_{}_{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_mfcc, n_hid, n_timb, loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction (whole dataset) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimbreEncoder(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "# model.to(device)\n",
    "\n",
    "data_tensor, label_tensor = map(torch.tensor, (data, labels))\n",
    "data_tensor = data_tensor.float(); label_tensor = label_tensor.long(); \n",
    "# data_tensor = data_tensor.to(device); label_tensor = label_tensor.to(device)\n",
    "\n",
    "correct = 0; wrong = 0;\n",
    "corrects = np.zeros(n_vowels); wrongs = np.zeros(n_vowels)\n",
    "predictions = np.zeros((n_vowels, n_vowels));\n",
    "for vowel_idx in range(n_vowels):\n",
    "    for pitch_idx in range(n_pitches):\n",
    "        for pid_idx in range(n_people):\n",
    "            wav_idx = flat_ref_idx(vowel_idx, pitch_idx, pid_idx)\n",
    "            for win_idx in range(n_windows):\n",
    "                data_idx = flat_data_idx(wav_idx, win_idx)\n",
    "                label = (label_tensor[data_idx]).item()\n",
    "                pred = (torch.argmax(model(data_tensor[data_idx]))).item()\n",
    "                \n",
    "                predictions[vowel_idx][pred] = predictions[vowel_idx][pred] + 1\n",
    "                if label == pred:\n",
    "                    correct = correct + 1\n",
    "                    corrects[vowel_idx] = corrects[vowel_idx] + 1\n",
    "                else:\n",
    "                    wrong = wrong + 1\n",
    "                    wrongs[vowel_idx] = wrongs[vowel_idx] + 1\n",
    "                    \n",
    "print(\"Total Accuracy: {}\"\n",
    "      .format(correct / (wrong + correct)))\n",
    "for vowel_idx in range(n_vowels):\n",
    "    print(\"Vowel: {}. Accuracy: {}. Most common pred: {}\"\n",
    "          .format(label_to_vowel[vowel_idx],\n",
    "                  corrects[vowel_idx] / (wrongs[vowel_idx] + corrects[vowel_idx]),\n",
    "                  label_to_vowel[np.argmax(predictions[vowel_idx])]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timbre-VAE - MFCC -> MFCC\n",
    "This takes MFCC, reduces dimensionality to a `n_timb` latent space, and attempts to recreate the MFCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hid = 10; n_timb = 4; lr = 1e-3; n_epochs = 10000; batch_size=22272\n",
    "\n",
    "# Training model \n",
    "model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "\n",
    "# Define loss - from pytorch VAE example.\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Available\" if torch.cuda.is_available() else \"GPU Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use GPU if possible (will run on CPU otherwise)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Move inputs to GPU (if possible)\n",
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "\n",
    "# Move the network to GPU (if possible)\n",
    "model.to(device) \n",
    "# Define optimizer \n",
    "# opt = optim.SGD(model.parameters(), lr=lr)\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Fit the model\n",
    "tic = time.time()\n",
    "loss = model.train_func(X_train, X_val, model, opt, loss_fn, batch_size=batch_size,\n",
    "                        epochs=n_epochs, print_graph = True)\n",
    "toc = time.time()\n",
    "print('Final loss: {}\\nTime taken: {}'.format(loss, toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "model_path = os.path.join(\"model_data\", \"TimbreVAE_{}_{}_{}_{}_{}_{}_{}.pt\"\n",
    "                          .format(lr, n_epochs, n_mfcc, n_hid, n_timb, batch_size, loss))\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved at {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved model, and using the model for prediction example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb, n_vowels=n_vowels)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "data_tensor = torch.tensor(data)\n",
    "data_tensor = data_tensor.float();\n",
    "\n",
    "wav_idx = flat_ref_idx(5, 5, 1)\n",
    "data_idx = flat_data_idx(wav_idx, 30)\n",
    "label = data_tensor[data_idx]\n",
    "pred = model(data_tensor[data_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Choosing `n_hid`, `n_timb`\n",
    "\n",
    "From running this, we find the results:\n",
    "Best `n_hid`: 10, Best `n_timb`: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d296f7d35d4c74acf7f46dab5925ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 6, n_timb: 4', max=2500.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 6, n_timb: 4, Final val loss: 12.756245229436063, Time taken: 24.036638021469116\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_6_4_12.756245229436063.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637a70bd5180424cb248d25fc7782aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 8, n_timb: 4', max=2500.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 8, n_timb: 4, Final val loss: 12.756556719198993, Time taken: 23.402870178222656\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_8_4_12.756556719198993.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4631374efbbf4dd393a26627aaf5e0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 8, n_timb: 6', max=2500.0, style=ProgressStyle(desâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 8, n_timb: 6, Final val loss: 12.757583793552442, Time taken: 24.187214136123657\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_8_6_12.757583793552442.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1234f6522da7490f8fea6ba030324f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 4', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 4, Final val loss: 12.755354256465518, Time taken: 23.62875461578369\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_4_12.755354256465518.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffe0fb4c21e4813a77fe5e903c54b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 6', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 6, Final val loss: 12.756701239224139, Time taken: 23.57104468345642\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_6_12.756701239224139.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3ffe45c5104ad68621a431a255b167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 10, n_timb: 8', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 10, n_timb: 8, Final val loss: 12.75628732264727, Time taken: 23.87569832801819\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_10_8_12.75628732264727.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfdae241df14f52b49ad43e45a25f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 4', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 4, Final val loss: 12.75652725395115, Time taken: 23.377437353134155\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_4_12.75652725395115.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24bdd35cf8da4354b09ed4e8ef78ad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 6', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 6, Final val loss: 12.756516029094827, Time taken: 24.132755041122437\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_6_12.756516029094827.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80524de2ec654ee5acf77ad615f62c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 8', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 8, Final val loss: 12.757957020025144, Time taken: 21.46918272972107\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_8_12.757957020025144.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcd9d9a0a3343c9a7652928ae3ff05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 12, n_timb: 10', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 12, n_timb: 10, Final val loss: 12.757127783764368, Time taken: 24.31379532814026\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_12_10_12.757127783764368.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8c7d87690647749f976883fcee2983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 4', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 4, Final val loss: 12.75653426948635, Time taken: 23.120083808898926\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_4_12.75653426948635.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549a86014cec4551b236af68296f1858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 6', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 6, Final val loss: 12.75579904139727, Time taken: 23.299814462661743\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_6_12.75579904139727.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f86b8c40f744bbb90b1b531ebcb35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 8', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 8, Final val loss: 12.757596421515805, Time taken: 24.524491548538208\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_8_12.757596421515805.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277dd35e7e734f41aa39c29bc911df44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 10', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 10, Final val loss: 12.756250841864224, Time taken: 24.066789388656616\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_10_12.756250841864224.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddccc7066410466cbdd1f752cbcedcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 14, n_timb: 12', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 14, n_timb: 12, Final val loss: 12.757847577676007, Time taken: 23.379464149475098\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_14_12_12.757847577676007.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77a68e0d1ad433f9c96b54812956090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 4', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 4, Final val loss: 12.75629153196839, Time taken: 23.141919136047363\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_4_12.75629153196839.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29807120aed403587af8f2212b5cfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 6', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 6, Final val loss: 12.75677841011135, Time taken: 23.408835649490356\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_6_12.75677841011135.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fbd525a318d4eb3858a38987e7fbc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 8', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 8, Final val loss: 12.757703057650861, Time taken: 23.205310583114624\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_8_12.757703057650861.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ad8405d7141fc9240f6a1c2167747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 10', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 10, Final val loss: 12.756217167295258, Time taken: 18.973145723342896\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_10_12.756217167295258.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb355397ada040c6a7f8bf467ec87881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 12', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 12, Final val loss: 12.756746138649426, Time taken: 18.91083335876465\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_12_12.756746138649426.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6f9708fc6b43bea0f2924e35c104b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 16, n_timb: 14', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 16, n_timb: 14, Final val loss: 12.757397180316092, Time taken: 19.097800970077515\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_16_14_12.757397180316092.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b581abac8004f38b745c698ece54d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 4', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 4, Final val loss: 12.756252244971265, Time taken: 18.403200149536133\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_4_12.756252244971265.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5c5069a36242f096ed402dba273109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 6', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 6, Final val loss: 12.755765366828305, Time taken: 20.536202669143677\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_6_12.755765366828305.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8c0252adfb4320b68b49d0684bb304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 8', max=2500.0, style=ProgressStyle(deâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 8, Final val loss: 12.756218570402298, Time taken: 18.79640245437622\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_8_12.756218570402298.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785fe56719e345f5b9911198c0e7a7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 10', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 10, Final val loss: 12.757287737966953, Time taken: 19.083486557006836\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_10_12.757287737966953.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69588899cbf04340b123a9f65a4a3c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 12', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 12, Final val loss: 12.757718491828305, Time taken: 19.04806160926819\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_12_12.757718491828305.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39afd0e0656a482c94401b5f7a4a7caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 14', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 14, Final val loss: 12.75823904454023, Time taken: 19.794686317443848\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_14_12.75823904454023.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b3e05805e848aa9c6decf75e2f9a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='n_hid: 18, n_timb: 16', max=2500.0, style=ProgressStyle(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "n_hid: 18, n_timb: 16, Final val loss: 12.756389749461206, Time taken: 18.548084497451782\n",
      "Model saved at model_data\\TimbreVAE_n_hid_n_timb_experiment_18_16_12.756389749461206.pt\n"
     ]
    }
   ],
   "source": [
    "# Define loss - from pytorch VAE example.\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "n_hid_candidates = [6, 8, 10, 12, 14, 16, 18]\n",
    "n_timb_candidates = [4, 6, 8, 10, 12, 14, 16]\n",
    "lr = 1e-3; n_epochs = 2500; batch_size=22272\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "losses = []\n",
    "for hid_idx in range(len(n_hid_candidates)):\n",
    "    for timb_idx in range(len(n_timb_candidates)):\n",
    "        if (hid_idx < timb_idx): continue\n",
    "        n_hid = n_hid_candidates[hid_idx]\n",
    "        n_timb = n_timb_candidates[timb_idx]\n",
    "        \n",
    "        # Training model \n",
    "        model = TimbreVAE(n_mfcc=n_mfcc, n_hid=n_hid, n_timb=n_timb)\n",
    "\n",
    "        X_train = X_train.to(device)\n",
    "        X_val = X_val.to(device)\n",
    "        model.to(device) \n",
    "        # opt = optim.SGD(model.parameters(), lr=lr)\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # Fit the model\n",
    "        tic = time.time()\n",
    "        loss, val_loss = model.train_func(X_train, X_val, model, opt, loss_fn, batch_size=batch_size,\n",
    "                                epochs=n_epochs, print_graph=False, desc=\"n_hid: {}, n_timb: {}\".format(n_hid, n_timb))\n",
    "        toc = time.time()\n",
    "        print('n_hid: {}, n_timb: {}, Final val loss: {}, Time taken: {}'.format(n_hid, n_timb, val_loss, toc - tic))\n",
    "        model_path = os.path.join(\"model_data\", \"TimbreVAE_n_hid_n_timb_experiment_{}_{}_{}.pt\"\n",
    "                                  .format(n_hid, n_timb, val_loss))\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"Model saved at {}\".format(model_path)) \n",
    "        losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_hid: 10, Best n_timb: 4\n",
      "[[12.756245229436063 list([6, 4])]\n",
      " [12.756556719198993 list([8, 4])]\n",
      " [12.757583793552442 list([8, 6])]\n",
      " [12.755354256465518 list([10, 4])]\n",
      " [12.756701239224139 list([10, 6])]\n",
      " [12.75628732264727 list([10, 8])]\n",
      " [12.75652725395115 list([12, 4])]\n",
      " [12.756516029094827 list([12, 6])]\n",
      " [12.757957020025144 list([12, 8])]\n",
      " [12.757127783764368 list([12, 10])]\n",
      " [12.75653426948635 list([14, 4])]\n",
      " [12.75579904139727 list([14, 6])]\n",
      " [12.757596421515805 list([14, 8])]\n",
      " [12.756250841864224 list([14, 10])]\n",
      " [12.757847577676007 list([14, 12])]\n",
      " [12.75629153196839 list([16, 4])]\n",
      " [12.75677841011135 list([16, 6])]\n",
      " [12.757703057650861 list([16, 8])]\n",
      " [12.756217167295258 list([16, 10])]\n",
      " [12.756746138649426 list([16, 12])]\n",
      " [12.757397180316092 list([16, 14])]\n",
      " [12.756252244971265 list([18, 4])]\n",
      " [12.755765366828305 list([18, 6])]\n",
      " [12.756218570402298 list([18, 8])]\n",
      " [12.757287737966953 list([18, 10])]\n",
      " [12.757718491828305 list([18, 12])]\n",
      " [12.75823904454023 list([18, 14])]\n",
      " [12.756389749461206 list([18, 16])]]\n"
     ]
    }
   ],
   "source": [
    "indices = [ [n_hid_candidates[hid_idx], n_timb_candidates[timb_idx]] \n",
    "            for hid_idx in range(len(n_hid_candidates))\n",
    "            for timb_idx in range(len(n_timb_candidates)) if (hid_idx >= timb_idx) ]\n",
    "best_hid, best_timb = indices[np.argmin(np.array(losses))]\n",
    "print(\"Best n_hid: {}, Best n_timb: {}\".format(best_hid, best_timb))\n",
    "print(np.array(list(zip(losses, indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
